---
title: "Lab 8"
author: "Ignacio Cabezudo"
date: "October 27, 2017"
output:
  pdf_document: default
  html_document: default
---

Using your own dataset (which may include more than one table) carry out the following data cleaning steps. Knit together the PDF document and commit both the Lab 8 RMD file and the PDF document to Git. Push the changes to GitHub so both documents are visible in your public GitHub repository. 

**Before you begin: as many of you have large datasets, you're going to want to select only the variables you're interested in utilizing for this project (ideally no more than twenty columns but perhaps much smaller) so you don't have R Studio's memory working on the entire dataset. The example code provided below can be modified to allow you to subset your data to only the variables you wish to use. First, read in your complete dataset and save it as** `data`. **Then, add the names of the variables you wish to use for your poster project to the** `select` **function, separated by commas. Run the two lines of code to save this new, smaller version of your data to** `data_subset`. **Use this smaller dataset to complete the rest of the lab**

```{r}
# Read in your data with the appropriate function
library(tidyverse)
library(readr)
library(ggplot2)

ACLED <- read_csv("ACLED-AFRICA-FULL.csv")
  
ACLED_parsed <- ACLED %>%
  select(EVENT_DATE, COUNTRY, YEAR, FATALITIES, LOCATION, LONGITUDE, LATITUDE)
ACLED_parsed <- dplyr::filter(ACLED_parsed, FATALITIES > 0) # Removes 0 Fatality events
#as.Date(ACLED_parsed$EVENT_DATE, "%m/%d/%Y") #make sure date is set correctly 
#clearly something is wrong with this. Not sure what's going on

```


1. To get a feel for its structure, look at the class, dimensions, column names, structure, and basic summary statistics of your data.

```{R}
summary(ACLED_parsed)
```

2. Preview the first and last 15 rows of your data. Is you dataset tidy? If not, what principles of tidy data does it seem to be violating?
```{R}
head(ACLED_parsed, 15)
tail(ACLED_parsed, 15)
```

I think my data is quite tidy!

3. Create a histogram for at least two variables you plan to focus on for your study. Describe what these plots show you about these variables. 

```{R}

ggplot(ACLED_parsed, aes(FATALITIES)) + geom_histogram(binwidth = 5) + scale_x_continuous(breaks = seq(0, 60, by = 25)) + labs(title = "Number of Fatalities per event", x = "Fatalities")

ggplot(ACLED_parsed, aes(YEAR)) + geom_histogram(binwidth = .5) + scale_x_continuous(breaks = seq(1990, 2020, by = 5)) + labs(title = "Fatality event by Year of occurence", x = "Year")

#I'm still working on what I want to do with these

#hist(ACLED_parsed$YEAR, xlim = c(1995, 2018), ylim = c(0, 150), breaks = 50)

```

4. Create at least one bivariate plot showing the relationship between two variables of interest. What does/do the(se) plot(s) tell you about the association between these two variables?

```{R}

#Still working on trying to make this look better
ggplot(ACLED_parsed, aes(YEAR, y = FATALITIES)) +
  geom_point(alpha = .075)  
  
  
  #plot(ACLED_parsed$YEAR, ACLED_parsed$FATALITIES)
```


5. Load the `tidyr` package. Do all of your columns correspond to variables? Do any columns represent multiple variables? If your answer is yes to either question, carry out the appropriate `tidyr` function (`gather()` or `spread()` respectively) to tidy your data. 

I'm happy with what I've got


6. Do any columns need to be separated into two or more? Do any columns need to be combined into one? If so, carry out the appropriate the appropriate `tidyr` function (`separate()` or `unite()` respectively) to tidy your data.  

Not needed for this dataset.

**At this stage each row in your data should represent one observation, each column should be a variable, and each table should be observational unit.** 

7. What is the class of each of the variables in your analysis? Are these classes appropriate for the type of measurement they purport to capture? Explain your reasoning. 

character, character, Int, Int, Chr, dbl, dbl   

The only one I'm concerned about is Date, as I'm considering breaking the data even further down to by month. In order to do this my idea was to use the commented out as.date() code in the initial cleaning phase, then later use that information to seperate things into monthly.  I'm not quite sure how to do that yet, but I don't think it's a difficult operation. 


8. Do any of your variables need to be coerced into a different data type? If so, carry out the appropriate coercion methods below. (This includes transformation of any date objects using the `lubridate` package)

Yes! Yes they do!! We just talked about that!

```{R}
#library(lubridate)



```

9. Are there any strings you need to manipulate for your analysis? If so, use the appropriate function from the [`stringr`](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html) package. 

No!


10. Do you have any missing values in your dataset? How many and how are they coded? **Be sure to look out for specific codebook values for missing values (i.e. `-1` for `NA`) as well as empty strings or other software-specific values for `NA`.** Don't worry about removing NAs yet - we'll tackle this question later once discern whether they're random or systematically distributed. 

No missing values!


11. Are there any special values in your dataset? If so, what are they and how do you think they got there? *The presence of special values is less likely if you haven't performed any data manipulation yet so you should remember to return to this step each time you carry out a mathematical transformation of any values in your dataset.*

I don't think so.

12. Create a boxplot of your data (you can create an individual boxplot for each variable if there are too many variables in your dataset to meaningfully visualize them all in one plot). Are there any outliers? If so, what are they and to which variable do they correspond? Do any of these outliers seem like obvious errors? If so, why? 
```{R}
boxplot(ACLED_parsed$FATALITIES)
boxplot(ACLED_parsed$YEAR)
```

13. For any outliers and/or obvious errors, what do you think is the best way to handle them (i.e. remove them entirely, run analyses including and excluding them and compare the results, manually change them to an appropriate measure of center, or something else?). 

I have one major outlier. Given that the data I'm looking at is about real people killed in real conflicts, I don't feel right about removing it, I may specifically address it. I haven't decided yet. 
